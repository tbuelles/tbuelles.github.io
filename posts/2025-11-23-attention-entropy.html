<!doctype html>
<html>


<head>
  <meta charset="utf-8" />
  <title>Attention Entropy Collapse — Tim-Henrik Buelles</title>
  <meta name="author" content="Tim-Henrik Buelles" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0" />
  <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <link rel="stylesheet" type="text/css" href="../home.css" />
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" />
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Noto+Serif:400,700,400italic|Open+Sans:700,400" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism.min.css" />
  <script defer src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-core.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-clike.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
  <script src="../home.js" defer></script>

</head>


<body>
  <div class="container">

    <div class="button">
    </div>
    
    <header class="site-header">
      <a class="link-bold back-left" href="/blog.html" aria-label="Blog"><i class="fa fa-arrow-left"></i> Blog</a>
      <div class="title-wrap">
        <h1>Attention Entropy Collapse</h1>
        <div class="post-meta">November 23, 2025</div>
      </div>
    </header>

    <!-- Body -->
    <div class="section">
      <h3>Intro</h3>
      <p>
        Attention entropy collapse is a failure mode in ML where attention logits converge to a small number of states, thus losing the ability to model the distribution.
        In practice, this is often patched with Layernorm / RMSNorm. I don't think this is the right approach long-term.
      </p>

      <h3>Entropy</h3>
      <p>
        Consider data \(x=(x_1, \ldots, x_n)\) from \(p(x)\). Recall entropy  \(-\mathbb{E}_{x \sim p}[\log p(x)]\).
        It is maximized by uniform distributions, and collapses to zero for delta distributions.
        Entropy is the Jensen gap between cross-entropy \(-\mathbb{E}_{x \sim p}[\log f(x)] \geq H(p)\) and KL divergence \(D_{KL}(p || f)\) for any model \(f\).

      <h3>Model</h3>
      <p>
        Let \(f_\theta\) be a parametric model for the \(n\) conditional distributions \(p(x_i | \bar x_i)\) where \(\bar x_i\) is the context \(x=(x_1, \ldots, \emptyset, \ldots, x_n)\).
        In ML, this is the typical subproblem of modeling \(p(x)\). Consider the simple attention model
        \begin{align*}
        q_i = g_i\left(W_q x_i\right)\,, \quad k_j &= g_j\left(W_k x_j\right), \quad v_j = W_v x_j \\[1em]
        \alpha_{ij} &= \frac{\exp(q_i \cdot k_j)}{\sum_{l=1}^n \exp(q_i \cdot k_l)} \\[1em]
        f_\theta(x)_i &= \sum_{j=1}^n \alpha_{ij} v_j
        \end{align*}

        The most commonly used \(g_i\) are composition of positional encodings \(g_i(y) = e^{2\pi i \gamma} y\) and normalization \(g_i(y) = \gamma * \frac{y - \bar \mu}{\bar \sigma} + \beta\).
      </p>

      <h2>GPU kernel</h2>

      <p>
        Attention is computed with "flash attention" which is tiling and online-softmax, see <a class="link-bold"href="https://github.com/Dao-AILab/flash-attention">https://github.com/Dao-AILab/flash-attention</a>.
        Any such implementation can be extended to compute entropy within the same kernel. This adds near zero overhead. It is particularly simple because it only touches the forward kernel.
        Below is an example using Triton, see <a class="link-bold" target="_blank" href="https://github.com/YOUR_USERNAME/YOUR_REPO">GitHub — Attention Entropy Kernel</a> for full code and usage.
      </p>

      <div class="code-wrap">
        <button class="copy-btn" data-target="kernel">Copy</button>
        <pre><code id="kernel" class="language-python">
@triton.jit
def attention_entropy_kernel(
    Q, K, S,  # input queries, keys, output entropies
    M, N, H,  # sizes: M=query length, N=key length, H=num heads
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr
):
    head_id = tl.program_id(0)    # which head are we processing
        </code></pre>
      </div>


    </div>

    <div class="footer"> Last updated Oct 12, 2025 </div>

  </div>

</body>

</html>
