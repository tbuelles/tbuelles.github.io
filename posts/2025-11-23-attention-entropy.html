<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Attention Entropy Collapse — Tim-Henrik Buelles</title>
  <meta name="author" content="Tim-Henrik Buelles" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0" />
  <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <link rel="stylesheet" type="text/css" href="../home.css" />
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" />
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Noto+Serif:400,700,400italic|Open+Sans:700,400" />
</head>

<body>
  <div class="container">

    <!-- Header -->
    <div class="section profile" style="align-items:center; position:relative;">
      <a class="link-bold back-left" href="../blog.html" aria-label="Back to blog">
        <i class="fa fa-arrow-left"></i> Back
      </a>
      <div class="profile-info">
        <h1>Attention Entropy Collapse</h1>
        <div class="post-meta" style="text-align:center; color:#666;">October 12, 2025</div>
      </div>
    </div>

    <!-- Body -->
    <div class="section">
      <h3>Intro</h3>
      <p>
        Attention entropy collapse is a failure mode in ML where attention logits become nearly one-hot, concentrating all probability mass on a single position.
        In practice, this is often patched with Layernorm / RMSNorm. I don't think this is the right long-term approach.
      </p>

      <h3>Entropy</h3>
      <p>
        Consider data \(x=(x_1, \ldots, x_n)\) from \(p(x)\). Recall entropy  \(-\mathbb{E}_{x \sim p}[\log p(x)]\).
        It is maximized by uniform distributions, and collapses to zero for delta distributions.
        Entropy is the Jensen gap between cross-entropy \(-\mathbb{E}_{x \sim p}[\log f(x)] \geq H(p)\) and KL divergence \(D_{KL}(p || f)\) for any model \(f\).

      <h3>Model</h3>
      <p>
        Let \(f_\theta\) be a parametric model for the \(n\) conditional distributions \(p(x_i | \bar x_i)\) where \(\bar x_i\) is the context \(x=(x_1, \ldots, \emptyset, \ldots, x_n)\).
        In ML, this is the typical subproblem of modeling \(p(x)\). Consider the simple attention model
        \begin{align*}
        q_i = g_i\left(W_q x_i\right)\,, \quad k_j &= g_j\left(W_k x_j\right), \quad v_j = W_v x_j \\[1em]
        \alpha_{ij} &= \frac{\exp(q_i \cdot k_j)}{\sum_{l=1}^n \exp(q_i \cdot k_l)} \\[1em]
        f_\theta(x)_i &= \sum_{j=1}^n \alpha_{ij} v_j
        \end{align*}

        The functions \(g_i\) are commonly used for positional encodings \(g_i(y) = e^{2\pi i \gamma} y\) or normalization \(g_i(y) = \frac{y - \bar \mu}{\bar \sigma}\).
      </p>

      <h2>GPU kernel</h2>

      <p>
        Attention is computed with "flash attention" which is tiling and online-softmax, see .
        Any such implementation can be extended to compute entropy within the same kernel. This adds near zero overhead. It is particularly simple because it only touches the forward kernel.
        Below is an example using Triton, see         <a class="link-bold" target="_blank" href="https://github.com/YOUR_USERNAME/YOUR_REPO">
          GitHub — Attention Entropy Monitor
        </a> for full code and usage.
      </p>

      <pre><code class="language-python">
        @triton.jit
        def attention_entropy_kernel(
            Q, K, S,  # input queries, keys, output entropies
            M, N, H,  # sizes: M=query length, N=key length, H=num heads
            BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr
        ):
            head_id = tl.program_id(0)    # which head are we processing
</code></pre>

    </div>

    <div class="footer"> Last updated Oct 12, 2025 </div>

  </div>

  <style>
    .back-left {
      position: absolute;
      left: 0.5rem;
      top: 0.5rem;
      text-decoration: none;
      color: inherit;
    }
    .back-left i { margin-right: 0.35rem; }
    pre {
      padding: 1rem;
      background: #f5f5f5;
      border-radius: 6px;
      overflow-x: auto;
      font-size: 0.95rem;
    }
    code { font-family: Menlo, monospace; }
  </style>

</body>
</html>
