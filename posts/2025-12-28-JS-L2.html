<!doctype html>
<html>

<head>
  <meta charset="utf-8" />
  <title>JS — Tim-Henrik Buelles</title>
  <meta name="author" content="Tim-Henrik Buelles" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0" />
  <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" />
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Noto+Serif:400,700,400italic|Open+Sans:700,400" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <link rel="stylesheet" type="text/css" href="../assets/css/home.css" />
  <script src="../assets/js/home.js" defer></script>
</head>

<body>
  <div class="container">

    <div class="button"></div>

    <header class="site-header">
      <a class="link-bold back-left" href="/blog.html" aria-label="Blog"><i class="fa fa-arrow-left"></i> Blog</a>
      <div class="title-wrap">
        <h1>Your model needs James&ndash;Stein</h1>
        <div class="post-meta"><span></span>December 28, 2025</div>
      </div>
    </header>

    <!-- Body -->
    <div class="section">
      <h3>Intro</h3>
      <p>
        The James&ndash;Stein estimator revolutionized statistics &ndash; but not machine learning. Well actually it did, but practitioners don't know.
        It shows that in high dimensions the naive unbiased estimator is inadmissible (bad).
        Shrinkage towards zero beats it uniformly.
        In this note, I review the estimation theory, connect James&ndash;Stein to L2 regularization,
        and explain why weight decay is a principled default.
        <!-- TODO: Connect to Bayesian? -->
      </p>
      <hr class="header-separator">

      <h2>Normal Means and Risk</h2>
      <p>
        Consider the normal means model
        \[
        y = \mu + \varepsilon,\quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I_k),\quad y,\mu \in \mathbb{R}^k.
        \]
        We observe data \(y\) and want to estimate \(\mu\). For now, we assume \(\varepsilon\) independent of \(\mu\) with fixed but unknown \(\sigma^2\).
        This is linear regression without features, i.e. design matrix \(X = I_k\).
        For an estimator \(\hat \mu(y)\), the frequentist risk under squared error is
        \[
        R(\mu, \hat \mu) = \mathbb{E}_\mu \left[ \|\hat \mu(y) - \mu\|_2^2 \right].
        \]
      </p>
      <p>
        The MLE estimator equals the least squares estimator: \(\hat \mu_{\text{MLE}} = y\).
        Its risk is constant in \(\mu\):
        \[
        R(\mu, \hat \mu_{\text{MLE}}) = \mathbb{E}_\mu \|y-\mu\|_2^2
        = \mathbb{E}\|\varepsilon\|_2^2 = k \sigma^2.
        \]
      </p>
      <hr class="header-separator">

      <h2>James–Stein Shrinkage</h2>
      <p>
        The James–Stein estimator shrinks the observation towards zero by a data-adaptive factor:
        \[
        \hat \mu_{\text{JS}}(y) = \left(1 - \frac{(k-2)\sigma^2}{\|y\|_2^2}\right) y.
        \]
        The positive-part version truncates negative shrinkage,
        \[
        \hat \mu_{\text{JS}}^+(y) = \left(1 - \frac{(k-2)\sigma^2}{\|y\|_2^2}\right)_+ y,
        \]
        and further improves risk.
        The striking fact is that for \(k \ge 3\),
        \[
        R(\mu, \hat \mu_{\text{JS}}) < R(\mu, \hat \mu_{\text{MLE}}) = k\sigma^2 \quad \text{for all } \mu.
        \]
      </p>
      <p>
        One way to see this is via Stein's unbiased risk estimate (SURE).
        Write \(\hat \mu(y) = y + g(y)\) with a weakly differentiable \(g\).
        Then
        \[
        R(\mu, \hat \mu) = k\sigma^2 + \mathbb{E}_\mu\left[ \|g(y)\|_2^2 + 2\sigma^2 \nabla \cdot g(y) \right].
        \]
        For James–Stein, \(g(y) = -\frac{(k-2)\sigma^2}{\|y\|_2^2} y\), which yields
        \[
        R(\mu, \hat \mu_{\text{JS}}) = k\sigma^2 - (k-2)^2 \sigma^4\, \mathbb{E}_\mu\left[\frac{1}{\|y\|_2^2}\right].
        \]
        The second term is strictly positive, so JS strictly dominates the MLE in risk.
      </p>
      <hr class="header-separator">

      <h2>Ridge / L2 as Constant Shrinkage</h2>
      <p>
        Now compare to L2 regularization in the same model.
        Solve the penalized least-squares problem
        \[
        \hat \mu_{\lambda} = \arg\min_\mu \; \|y-\mu\|_2^2 + \lambda \|\mu\|_2^2.
        \]
        The first-order condition gives
        \[
        (1+\lambda)\hat \mu_{\lambda} = y \quad \Rightarrow \quad \hat \mu_{\lambda} = \frac{1}{1+\lambda} y.
        \]
        So ridge is a constant shrinkage rule: every coordinate is scaled by the same factor,
        regardless of the signal strength.
      </p>
      <p>
        James–Stein is the same qualitative idea but with a data-adaptive shrinkage factor
        \(1 - (k-2)\sigma^2/\|y\|_2^2\).
        Ridge is a fixed compromise; JS is a rule that shrinks more when the overall signal looks small.
      </p>
      <hr class="header-separator">

      <h2>From Estimators to Training: L2 vs Weight Decay</h2>

      <h3>Objective-level L2</h3>
      <p>
        In ML, L2 regularization adds a quadratic penalty to the loss:
        \[
        \min_\theta \; L(\theta) + \frac{\lambda}{2}\|\theta\|_2^2.
        \]
        This is equivalent to MAP estimation under a Gaussian prior \(\theta \sim \mathcal{N}(0, \lambda^{-1} I)\).
      </p>
      <hr class="header-separator">

      <h3>Update-level Weight Decay</h3>
      <p>
        With SGD, L2 regularization yields the update
        \[
        \theta_{t+1} = \theta_t - \eta \nabla L(\theta_t) - \eta \lambda \theta_t
        = (1-\eta\lambda)\theta_t - \eta \nabla L(\theta_t).
        \]
        The multiplicative term \((1-\eta\lambda)\) is exactly weight decay.
        For adaptive methods (Adam, RMSProp), an L2 penalty and decoupled weight decay are not the same;
        AdamW implements the decoupled form to preserve true shrinkage.
      </p>
      <hr class="header-separator">

      <h2>Why James–Stein Argues for Weight Decay</h2>
      <p>
        The normal-means model is crude, but its message survives in high-dimensional ML:
      </p>
      <ul>
        <li>
          In large \(k\), even an unbiased estimator of many parameters is improvable by shrinkage.
          JS says the naive solution is dominated.
        </li>
        <li>
          Weight decay is the simplest shrinkage rule: it pulls parameters toward zero every step,
          just like ridge in the normal means model.
        </li>
        <li>
          When the data are weak (small effective \(\|y\|_2\) or high noise), shrinkage should be stronger.
          JS achieves this adaptively; in deep nets we approximate it with constant decay plus early stopping.
        </li>
        <li>
          The risk reduction from JS is uniform in \(\mu\); the improvement is not a prior artifact.
          That is a strong argument for weight decay as a default, even when you are not fully Bayesian.
        </li>
      </ul>
      <p>
        The practical takeaway is not that L2 equals JS, but that the default of “no shrinkage”
        is statistically indefensible in high dimensions.
        Weight decay is the simplest, scalable, and optimizer-friendly way to encode that lesson.
      </p>
      <hr class="header-separator">

      <h2>References</h2>
      <p>
        Stein (1956), “Inadmissibility of the usual estimator for the mean of a multivariate normal distribution.”
        <br>
        James and Stein (1961), “Estimation with quadratic loss.”
        <br>
        Tikhonov (1963) / Hoerl and Kennard (1970), ridge regression.
        <br>
        Loshchilov and Hutter (2019), “Decoupled Weight Decay Regularization (AdamW).”
      </p>

    </div>

    <div class="footer"> Last updated December 28, 2025</div>

  </div>
</body>

</html>
