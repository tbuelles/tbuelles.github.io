<!doctype html>
<html>


<head>
  <meta charset="utf-8" />
  <title>Attention Entropy Collapse — Tim-Henrik Buelles</title>
  <meta name="author" content="Tim-Henrik Buelles" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0" />
  <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <link rel="stylesheet" type="text/css" href="../home.css" />
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" />
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Noto+Serif:400,700,400italic|Open+Sans:700,400" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism.min.css" />
  <script defer src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-core.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-clike.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
  <script src="../home.js" defer></script>

</head>


<body>
  <div class="container">

    <div class="button">
    </div>
    
    <header class="site-header">
      <a class="link-bold back-left" href="/blog.html" aria-label="Blog"><i class="fa fa-arrow-left"></i> Blog</a>
      <div class="title-wrap">
        <h1>Online Attention Entropy</h1>
        <div class="post-meta">November 23, 2025</div>
      </div>
    </header>

    <!-- Body -->
    <div class="section">
      <h3>Intro</h3>
      <p>
        Attention entropy collapse is a failure mode in ML where attention logits converge to a small number of states, thus losing the ability to model the distribution.
        In practice, this is often patched with Layernorm / RMSNorm.<span class="footnote-ref" data-note="I don't think this is the right approach long-term."></span>
        I discuss how to monitor and compute the attention entropy online with the same GPU kernel.
      </p>

      <h3>Entropy</h3>
      <p>
        Consider data \(x=(x_1, \ldots, x_n)\) from \(p(x)\). Recall entropy  \(-\mathbb{E}_{x \sim p}[\log p(x)]\).
        It is maximized by uniform distributions, and collapses to zero for delta distributions.
        Entropy is the Jensen gap between cross-entropy \(-\mathbb{E}_{x \sim p}[\log f(x)] \geq H(p)\) and KL divergence \(D_{KL}(p || f)\) for any model \(f\).

      <h3>Model</h3>
      <p>
        Let \(f_\theta\) be a parametric model for the \(n\) conditional distributions \(p(x_i | \bar x_i)\) where \(\bar x_i\) is the context \(x=(x_1, \ldots, \emptyset, \ldots, x_n)\).
        In ML, this is the typical subproblem of modeling \(p(x)\). Consider the simple attention model
        \begin{align*}
        q_i = g_i\left(W_q x_i\right)\,, \quad k_j &= g_j\left(W_k x_j\right), \quad v_j = W_v x_j \\[1em]
        \alpha_{ij} &= \frac{\exp(q_i \cdot k_j)}{\sum_{l=1}^n \exp(q_i \cdot k_l)} \\[1em]
        f_\theta(x)_i &= \sum_{j=1}^n \alpha_{ij} v_j
        \end{align*}

        The most commonly used \(g_i\) are compositions of positional encodings \(g_i(y) = e^{2\pi i \gamma} y\) and normalization \(g_i(y) = \gamma * \frac{y - \bar \mu}{\bar \sigma} + \beta\).
      </p>

      <h2>Fused Attention</h2>

      <p>
        Attention is computed with tiling and online softmax, see <a class="link-bold" href="https://arxiv.org/pdf/2205.14135">https://arxiv.org/pdf/2205.14135</a>. I will use Triton jargon to recall the algorithm (I find this more intuitive than CUDA).
        The work is divided into parallel programs each processing a block of queries \(q_i\) of size <code>BLOCK_M</code>. Note that from now on \(i\) denotes a block of indices rather than a single index.
        Each program loops over tiles of keys and values to compute online softmax by updating accumulators \(m_i, l_i, acc_i\) as
        \begin{align*}
        s_{ij} &\leftarrow q_i \cdot k_j \\
        m_{ij} &\leftarrow \max(m_i, s_{ij}) \\
        l_{ij} &\leftarrow l_i * \exp(m_i - m_{ij}) + \sum_{\text{tile}} \exp(s_{ij} - m_i) \\
        acc_i &\leftarrow acc_i * \frac{l_i}{l_{ij}} + \sum_{\text{tile}} \exp(s_{ij} - m_i) v_j\\
        m_i &\leftarrow \max(m_i, m_{ij}) \\
        l_i &\leftarrow l_{ij}\\
        \end{align*}
        This computes the max \(m_i\) of dot products across all tiles and the partition function \(l_i = \sum_{j=1}^n \exp(s_{ij} - m_i)\). The output is \(f_\theta(x)_i = acc_i / l_i\).
        This algorithm can be extended to compute the entropy online with near zero additional overhead. 
    </p>

      <h2>Online Entropy</h2>
      We can rearrange the entropy computation as
      \begin{align*}
      H(p_i) &= -\sum_{j=1}^n p_{ij} \log p_{ij} \\
      &= -\sum_{j=1}^n p_{ij} (s_{ij} - m_i - \log l_i) \\
      &= m_i + \log l_i - \sum_{j=1}^n \exp(s_{ij} - m_i)s_{ij}
      \end{align*}
      The first term \(m_i + \log l_i\) is already computed online. The last term can be computed online as well using an additional accumulator for \(r_i = \sum_j \exp(s_{ij} - m_i) s_{ij}\) and update as:
        \begin{align*}
        r_i &\leftarrow r_i * \frac{l_i}{l_{ij}} + \sum_{\text{tile}} \exp(s_{ij} - m_i) s_{ij} \\
        \end{align*}
      The final result for the entropy is then \(H(p_i) = m_i + \log l_i - r_i\).

      <h2>GPU kernel</h2>
    Any implementation of fused attention can be extended as above. It is particularly simple in this case because it only concerns the forward kernel.
    Below is an extension of <a class="link-bold" href="https://github.com/triton-lang/triton/blob/main/python/tutorials/06-fused-attention.py">OpenAI's fused attention</a>, see <a class="link-bold" target="_blank" href="https://github.com/YOUR_USERNAME/YOUR_REPO">GitHub — Attention Entropy Kernel</a> for full code and usage.
      <div class="code-wrap">
        <button class="copy-btn" data-target="kernel">Copy</button>
        <pre><code id="kernel" class="language-python">
@triton.jit
def _attn_fwd(sm_scale, M,
    ENTROPY,  # NEW
    Z, H, desc_q, desc_k, desc_v, desc_o, N_CTX,
    COMPUTE_ENTROPY: tl.constexpr, # NEW
    HEAD_DIM: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    FP8_OUTPUT: tl.constexpr,
    STAGE: tl.constexpr,
    warp_specialize: tl.constexpr,
    IS_HOPPER: tl.constexpr,
):
    ...
    offset_y = off_z * (N_CTX * H) + off_h * N_CTX
    qo_offset_y = offset_y + start_m * BLOCK_M
    # initialize offsets
    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = tl.arange(0, BLOCK_N)
    # initialize pointer to m and l
    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")
    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0
    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)
    ### New
    if COMPUTE_ENTROPY:
        r_i = tl.zeros([BLOCK_M], dtype=tl.float32)
    ###
    ...

    # epilogue
    m_i += tl.math.log2(l_i)
    acc = acc / l_i[:, None]
    m_ptrs = M + off_hz * N_CTX + offs_m
    tl.store(m_ptrs, m_i)
    desc_o.store([qo_offset_y, 0], acc.to(dtype))

    if COMPUTE_ENTROPY:
    entropy_ptrs = ENTROPY + off_hz * N_CTX + offs_m
        tl.store(entropy_ptrs, m_i - r_i)

        </code></pre>
      </div>
      

      <div class="code-wrap">
        <button class="copy-btn" data-target="kernel">Copy</button>
        <pre><code id="kernel" class="language-python">
@triton.jit
def _attn_fwd_inner(
    acc, l_i, m_i,
    # New
    r_i,
    # ---
    q,
    desc_k, desc_v,
    offset_y, dtype: tl.constexpr, start_m, qk_scale,
    COMPUTE_ENTROPY: tl.constexpr,  # NEW
    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,
    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,
    N_CTX: tl.constexpr, warp_specialize: tl.constexpr, IS_HOPPER: tl.constexpr
):
    # range of values handled by this stage
    if STAGE == 1:
        lo, hi = 0, start_m * BLOCK_M
    elif STAGE == 2:
        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M
        lo = tl.multiple_of(lo, BLOCK_M)
    # causal = False
    else:
        lo, hi = 0, N_CTX
    offsetk_y = offset_y + lo
    if dtype == tl.float8e5:
        offsetv_y = offset_y * HEAD_DIM + lo
    else:
        offsetv_y = offset_y + lo
    # loop over k, v and update accumulator
    for start_n in tl.range(lo, hi, BLOCK_N, warp_specialize=warp_specialize):
        start_n = tl.multiple_of(start_n, BLOCK_N)
        # -- compute qk ----
        k = desc_k.load([offsetk_y, 0]).T
        qk = tl.dot(q, k)
        if STAGE == 2:
            mask = offs_m[:, None] >= (start_n + offs_n[None, :])
            qk = qk * qk_scale + tl.where(mask, 0, -1.0e6)
            m_ij = tl.maximum(m_i, tl.max(qk, 1))
            ### REMOVE
            qk -= m_ij[:, None]
            # ---
            
        else:
            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)
            ### REMOVE
            qk = qk * qk_scale - m_ij[:, None]
            # ---
            ### New
            qk = qk * qk_scale
            # ---
        p = tl.math.exp2(qk)
        # -- compute correction factor
        alpha = tl.math.exp2(m_i - m_ij)
        l_ij = tl.sum(p, 1)
        # -- update output accumulator --
        if not IS_HOPPER and warp_specialize and BLOCK_M == 128 and HEAD_DIM == 128:
            BM: tl.constexpr = acc.shape[0]
            BN: tl.constexpr = acc.shape[1]
            acc0, acc1 = acc.reshape([BM, 2, BN // 2]).permute(0, 2, 1).split()
            acc0 = acc0 * alpha[:, None]
            acc1 = acc1 * alpha[:, None]
            acc = tl.join(acc0, acc1).permute(0, 2, 1).reshape([BM, BN])
        else:
            acc = acc * alpha[:, None]
                    # prepare p and v for the dot
        if dtype == tl.float8e5:
            v = desc_v.load([0, offsetv_y]).T
        else:
            v = desc_v.load([offsetv_y, 0])
        p = p.to(dtype)
        # note that this non transposed v for FP8 is only supported on Blackwell
        acc = tl.dot(p, v, acc)
        # update m_i and l_i
        # place this at the end of the loop to reduce register pressure
        l_i = l_i * alpha + l_ij
        m_i = m_ij
        # --- NEW ---
        if COMPUTE_ENTROPY:
            r_i = tl.dot(p, qk, l_i / l_ij * r_i)
        # ---
        offsetk_y += BLOCK_N
        offsetv_y += BLOCK_N
    return acc, l_i, m_i
        

        </code></pre>
      </div>


    </div>

    <div class="footer"> Last updated Oct 12, 2025 </div>

  </div>

</body>

</html>
