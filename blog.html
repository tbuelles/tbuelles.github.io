<!doctype html>
<html>
<head>
    <meta charset="utf-8" />
    <title>Blog — Tim-Henrik Buelles</title>
    <meta name="author" content="Tim-Henrik Buelles" />
    <meta name="viewport" content="width=device-width; initial-scale=1.0" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Noto+Serif:400,700,400italic|Open+Sans:700,400" />
    <link rel="stylesheet" type="text/css" href="./assets/css/home.css" />
    <script src="./assets/js/home.js" defer></script>
</head>

<body>
  <div class="container">

    <!-- Header / back link -->
    <header class="site-header">
      <a class="link-bold back-left" href="./index.html" aria-label="Home"><i class="fa fa-arrow-left"></i>Home</a>

      <div class="title-wrap">
        <h1>Technical Stuff</h1>
      </div>
    </header>

    <!-- Intro -->
    <div class="section">
      <p class="post-abstract">
        Thoughts on research and applied stats. Experiences as a mathematician in industry. Opinions are my own.
      </p>
    </div>

    <!-- Posts list -->
    <div class="section posts-list">
    
    <!-- Post 2 -->
    <article class="post-summary">
    <a class="post-link" href="posts/2025-12-28-JS-L2.html">
        <h2 class="post-title">Your model needs James–Stein</h2>
        <div class="post-meta"><span>December 28, 2025</span></div>
        <p class="post-abstract">
        The James–Stein estimator shows that the maximum likelihood estimator is inadmissible for multivariate normal means.
        This leads to a surprising result: shrinkage estimators can outperform the MLE.
        In machine learning, this idea connects to L2 regularization.
        </p>
    </a>
    </article>

    <!-- Post 1 -->
    <article class="post-summary">
    <a class="post-link" href="posts/2025-11-23-online-attention-entropy.html">
        <h2 class="post-title">Your model needs entropy</h2>
        <div class="post-meta"><span>November 26, 2025</span></div>
        <p class="post-abstract">
        Attention entropy collapse is a failure mode in ML where attention logits converge to a small number of states.
        In practice, this is often poorly monitored and patched with normalization.
        In this note, I discuss how to extend the attention algorithm to compute entropy online with the same GPU kernel.
        </p>
    </a>
    </article>

    </div>

    <div class="footer"> Last updated December 28, 2025</div>

  </div>

</body>
</html>
