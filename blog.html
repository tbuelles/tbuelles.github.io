<!doctype html>
<html>
<head>
    <meta charset="utf-8" />
    <title>Blog — Tim-Henrik Buelles</title>
    <meta name="author" content="Tim-Henrik Buelles" />
    <meta name="viewport" content="width=device-width; initial-scale=1.0" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Noto+Serif:400,700,400italic|Open+Sans:700,400" />
    <link rel="stylesheet" type="text/css" href="./assets/css/home.css" />
    <script src="./assets/js/home.js" defer></script>
</head>

<body>
  <div class="container">

    <!-- Header / back link -->
    <header class="site-header">
      <a class="link-bold back-left" href="./index.html" aria-label="Home"><i class="fa fa-arrow-left"></i>Home</a>

      <div class="title-wrap">
        <h1>Technical Stuff</h1>
      </div>
    </header>

    <!-- Intro -->
    <div class="section">
      <p class="post-abstract">
        Thoughts on research, applied stats, and machine learning. Experiences as a mathematician in industry.
      </p>
    </div>

    <!-- Posts list -->
    <div class="posts-list">
    <div class="section">
        <!-- Post 2 -->
        <article class="post-summary">
        <a class="post-link" href="posts/2025-12-28-james-stein.html">
            <h2 class="post-title">James–Stein</h2>
            <div class="post-meta"><span>December 28, 2025</span></div>
            <p class="post-abstract">
    James&ndash;Stein changed statistical inference forever &ndash; ML didn't get the memo. At least many practitioners seem oddly unaware of it.
            In 1955, Charles Stein found that maximum likelihood estimators for Gaussian models are inadmissible (bad).
            Shrinkage towards zero beats it uniformly (good).
            In this note, I review the estimation theory and connect James&ndash;Stein to two popular methods: L2 regularization and weight decay.
            </p>
        </a>
        </article>
    </div>

    <!-- Post 1 -->
    <div class="section">
        <article class="post-summary">
        <a class="post-link" href="posts/2025-11-23-online-attention-entropy.html">
            <h2 class="post-title">Online Attention Entropy</h2>
            <div class="post-meta"><span>November 26, 2025</span></div>
            <p class="post-abstract">
            Attention entropy collapse is a failure mode in ML where attention logits converge to a small number of states.
            In practice, this is often poorly monitored and patched with normalization.
            In this note, I discuss how to extend the attention algorithm to compute entropy online with the same GPU kernel.
            </p>
        </a>
        </article>
    </div>

    </div>

    <div class="footer"> Last updated February 1, 2026</div>

  </div>

</body>
</html>
